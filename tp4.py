# -*- coding: utf-8 -*-
"""tp4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12_pBUsajKum0vdTmbNBDjH7smb8egnQj

#**Rapport Tp4 Analyse de données discriminante**

**Réalisé par:**

1. Hanafi Yasmine Lina Amina
2. Kebache Amira
3. Hamza Lydia

#**1. Importation des bibliothèques**
"""

import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
df = pd.read_csv('framingham_heart_disease.csv')

df

"""#**2. Verification des valeurs manquantes**


"""

series = pd.isnull(df['cigsPerDay'])

"""vérifier si les valeurs de la colonne 'cigsPerDay' du DataFrame df sont manquantes (NaN).

df['cigsPerDay'] :

Extrait la colonne nommée 'cigsPerDay' du DataFrame df.
Cette colonne représente probablement le nombre moyen de cigarettes fumées par jour pour chaque individu dans l'étude.

pd.isnull() :

Fonction de Pandas qui retourne un Series de valeurs booléennes (True/False) indiquant si une valeur est NaN (Not a Number) ou non.
Elle vérifie les données manquantes pour chaque ligne de la colonne sélectionnée.
series :

Stocke le résultat de pd.isnull(df['cigsPerDay']), qui est une série de type booléen.
"""

data = df.drop(['currentSmoker','education'], axis = 'columns')
cigs = data['cigsPerDay']
cig = cigs.mean()
integer_value = math.floor(cig)
cigs.fillna(integer_value, inplace = True)
data.dropna( axis = 0, inplace = True)
data

Heart_Attack = data[data.TenYearCHD == 1]
No_Heart_Attack = data[data.TenYearCHD == 0]
final = data.drop(['diaBP','BMI','heartRate'], axis = 'columns')
final

# Identify numeric columns (excluding the target variable 'TenYearCHD')
numeric_columns = final.drop(columns=['TenYearCHD']).columns

# Group by 'TenYearCHD' and calculate the mean for numeric columns
centroids = final.groupby('TenYearCHD')[numeric_columns].mean()

# Display the centroids
print("Centroids for each class (TenYearCHD):")
print(centroids)

from scipy.spatial.distance import mahalanobis
import numpy as np

# Select numeric columns (excluding 'TenYearCHD')
numeric_data = final.drop(columns=['TenYearCHD']).values

# Calculate the mean vector
mean_vector = np.mean(numeric_data, axis=0)

# Calculate the covariance matrix
cov_matrix = np.cov(numeric_data, rowvar=False)

# Invert the covariance matrix
cov_matrix_inv = np.linalg.inv(cov_matrix)

# Calculate Mahalanobis distances for all rows
mahalanobis_distances = []
for i in range(len(numeric_data)):
    x = numeric_data[i]
    md = mahalanobis(x, mean_vector, cov_matrix_inv)
    mahalanobis_distances.append(md)

# Add distances to the dataset
final['MahalanobisDistance'] = mahalanobis_distances

# Display updated dataset
print(final.head())

import numpy as np

# Separate the classes
class_0 = final[final['TenYearCHD'] == 0].drop(columns=['TenYearCHD']).values
class_1 = final[final['TenYearCHD'] == 1].drop(columns=['TenYearCHD']).values

# Calculate mean vectors
mean_0 = np.mean(class_0, axis=0)
mean_1 = np.mean(class_1, axis=0)

# Calculate within-class scatter matrix (S_W)
S_W = np.cov(class_0, rowvar=False) * (len(class_0) - 1) + np.cov(class_1, rowvar=False) * (len(class_1) - 1)

# Calculate between-class scatter matrix (S_B)
overall_mean = np.mean(final.drop(columns=['TenYearCHD']).values, axis=0)
mean_diff = (mean_1 - mean_0).reshape(-1, 1)  # Difference between class means
S_B = len(class_0) * np.dot(mean_diff, mean_diff.T) + len(class_1) * np.dot(mean_diff, mean_diff.T)

# Compute Fisher's Linear Discriminant
eigenvalues, eigenvectors = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))

# Get the eigenvector corresponding to the largest eigenvalue
idx = np.argmax(eigenvalues)
fisher_coefficients = eigenvectors[:, idx].real

# Print Fisher's coefficients
print("Fisher's Linear Discriminant Coefficients:")
print(fisher_coefficients)

# Features and target
features = final.drop(columns=['TenYearCHD']).values  # Exclude target
target = final['TenYearCHD'].values  # Target variable

# Calculate Z (discriminant variable)
final['Z'] = features @ fisher_coefficients

# Calculate the classification threshold
class_means = final.groupby('TenYearCHD')['Z'].mean()
threshold = class_means.mean()

# Classify based on the threshold
final['Predicted_Y'] = (final['Z'] > threshold).astype(int)

# Calculate accuracy and error rate
accuracy = (final['Predicted_Y'] == final['TenYearCHD']).mean()
error_rate = 1 - accuracy

# Display results
print("Classification accuracy:", accuracy)
print("Error rate:", error_rate)

# Optional: Display the first few rows with results
print(final[['Z', 'TenYearCHD', 'Predicted_Y']].head())

from sklearn.preprocessing import LabelEncoder

# Select features (X) and target (y)
X = final.iloc[:, :-1].values  # All columns except the last one
y = final['TenYearCHD'].values  # Last column is the target

# Encode the target variable (if necessary)
le = LabelEncoder()
y = le.fit_transform(y)  # Transform the target variable into numeric form (if not already numeric)

# Verify shapes
print("Shape of X:", X.shape)
print("Shape of y:", y.shape)

# Optional: Preview X and y
print("First 5 rows of X:\n", X[:5])
print("First 5 values of y:\n", y[:5])

import seaborn as sns
import matplotlib.pyplot as plt

# Create a pair plot for the dataset
ax = sns.pairplot(
    final,
    hue='TenYearCHD',  # Use 'TenYearCHD' to differentiate classes
    markers=["o", "s"],  # Different markers for the classes
    diag_kind='kde',  # Kernel Density Estimation for diagonal plots
    plot_kws={'alpha': 0.6}  # Transparency for better visualization
)

# Add a title to the pair plot
plt.suptitle("Pair Plot of Heart Disease Dataset", y=1.02)

# Adjust the legend position
sns.move_legend(
    ax, "lower center",
    bbox_to_anchor=(.5, 1), ncol=2, title="TenYearCHD", frameon=False
)

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

features = final.columns[:-1]

# Create the histograms
plt.figure(figsize=(12, 6))  # Adjust size as needed
for i, feature in enumerate(features):
    if i+1 < 7:
      plt.subplot(2, 3, i + 1)  # Create subplots (adjust rows/cols based on number of features)
      sns.histplot(data=final, x=feature, hue='TenYearCHD', kde=True, palette='Set2', alpha=0.6)
      plt.title(f'{feature} Distribution')

# Adjust layout
plt.tight_layout()
plt.show()

correlation_matrix = final.corr(numeric_only = True)
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("matrice de correlation")
plt.show()

